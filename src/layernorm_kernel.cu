#include "includes/block_reduce.h"
#include "includes/kernels.h"
#include "includes/cuda_util.h"

#include <cooperative_groups.h>
#include <cstddef>

namespace cg = cooperative_groups;
namespace lightseq {
namespace cuda {

const float LN_EPSILON = 1e-8f;
#define TILE_DIM 32

/* reciprocals of integer multiples of 4 through 4096 */
__constant__ float reciprocals[4097] = {
                      0.f,                   0.25f,                  0.125f,    0.08333333333333333f,                 0.0625f,                   0.05f,   0.041666666666666664f,    0.03571428571428571f,                0.03125f,   0.027777777777777776f, //    0 -   36
                   0.025f,   0.022727272727272728f,   0.020833333333333332f,   0.019230769230769232f,   0.017857142857142856f,   0.016666666666666666f,               0.015625f,   0.014705882352941176f,   0.013888888888888888f,   0.013157894736842105f, //   40 -   76
                  0.0125f,   0.011904761904761904f,   0.011363636363636364f,   0.010869565217391304f,   0.010416666666666666f,                   0.01f,   0.009615384615384616f,   0.009259259259259259f,   0.008928571428571428f,   0.008620689655172414f, //   80 -  116
    0.008333333333333333f,   0.008064516129032258f,              0.0078125f,   0.007575757575757576f,   0.007352941176470588f,   0.007142857142857143f,   0.006944444444444444f,   0.006756756756756757f,   0.006578947368421052f,    0.00641025641025641f, //  120 -  156
                 0.00625f,   0.006097560975609756f,   0.005952380952380952f,   0.005813953488372093f,   0.005681818181818182f,   0.005555555555555556f,   0.005434782608695652f,   0.005319148936170213f,   0.005208333333333333f,    0.00510204081632653f, //  160 -  196
                   0.005f,   0.004901960784313725f,   0.004807692307692308f,  0.0047169811320754715f,   0.004629629629629629f,   0.004545454545454545f,   0.004464285714285714f,  0.0043859649122807015f,   0.004310344827586207f,    0.00423728813559322f, //  200 -  236
    0.004166666666666667f,   0.004098360655737705f,   0.004032258064516129f,   0.003968253968253968f,             0.00390625f,  0.0038461538461538464f,   0.003787878787878788f,  0.0037313432835820895f,   0.003676470588235294f,  0.0036231884057971015f, //  240 -  276
   0.0035714285714285713f,  0.0035211267605633804f,   0.003472222222222222f,   0.003424657534246575f,  0.0033783783783783786f,  0.0033333333333333335f,   0.003289473684210526f,   0.003246753246753247f,   0.003205128205128205f,  0.0031645569620253164f, //  280 -  316
                0.003125f,  0.0030864197530864196f,   0.003048780487804878f,  0.0030120481927710845f,   0.002976190476190476f,  0.0029411764705882353f,  0.0029069767441860465f,  0.0028735632183908046f,   0.002840909090909091f,  0.0028089887640449437f, //  320 -  356
    0.002777777777777778f,  0.0027472527472527475f,   0.002717391304347826f,   0.002688172043010753f,  0.0026595744680851063f,   0.002631578947368421f,  0.0026041666666666665f,   0.002577319587628866f,   0.002551020408163265f,  0.0025252525252525255f, //  360 -  396
                  0.0025f,  0.0024752475247524753f,  0.0024509803921568627f,  0.0024271844660194173f,   0.002403846153846154f,   0.002380952380952381f,  0.0023584905660377358f,   0.002336448598130841f,  0.0023148148148148147f,  0.0022935779816513763f, //  400 -  436
   0.0022727272727272726f,  0.0022522522522522522f,   0.002232142857142857f,  0.0022123893805309734f,  0.0021929824561403508f,   0.002173913043478261f,  0.0021551724137931034f,   0.002136752136752137f,    0.00211864406779661f,  0.0021008403361344537f, //  440 -  476
   0.0020833333333333333f,   0.002066115702479339f,  0.0020491803278688526f,  0.0020325203252032522f,  0.0020161290322580645f,                  0.002f,   0.001984126984126984f,   0.001968503937007874f,            0.001953125f,   0.001937984496124031f, //  480 -  516
   0.0019230769230769232f,  0.0019083969465648854f,   0.001893939393939394f,  0.0018796992481203006f,  0.0018656716417910447f,   0.001851851851851852f,   0.001838235294117647f,  0.0018248175182481751f,  0.0018115942028985507f,  0.0017985611510791368f, //  520 -  556
   0.0017857142857142857f,  0.0017730496453900709f,  0.0017605633802816902f,  0.0017482517482517483f,   0.001736111111111111f,  0.0017241379310344827f,  0.0017123287671232876f,  0.0017006802721088435f,  0.0016891891891891893f,  0.0016778523489932886f, //  560 -  596
   0.0016666666666666668f,  0.0016556291390728477f,   0.001644736842105263f,  0.0016339869281045752f,  0.0016233766233766235f,  0.0016129032258064516f,  0.0016025641025641025f,  0.0015923566878980893f,  0.0015822784810126582f,  0.0015723270440251573f, //  600 -  636
               0.0015625f,  0.0015527950310559005f,  0.0015432098765432098f,  0.0015337423312883436f,   0.001524390243902439f,  0.0015151515151515152f,  0.0015060240963855422f,  0.0014970059880239522f,   0.001488095238095238f,  0.0014792899408284023f, //  640 -  676
   0.0014705882352941176f,  0.0014619883040935672f,  0.0014534883720930232f,   0.001445086705202312f,  0.0014367816091954023f,  0.0014285714285714286f,  0.0014204545454545455f,  0.0014124293785310734f,  0.0014044943820224719f,  0.0013966480446927375f, //  680 -  716
    0.001388888888888889f,  0.0013812154696132596f,  0.0013736263736263737f,   0.001366120218579235f,   0.001358695652173913f,  0.0013513513513513514f,  0.0013440860215053765f,   0.001336898395721925f,  0.0013297872340425532f,  0.0013227513227513227f, //  720 -  756
   0.0013157894736842105f,  0.0013089005235602095f,  0.0013020833333333333f,  0.0012953367875647669f,   0.001288659793814433f,   0.001282051282051282f,  0.0012755102040816326f,  0.0012690355329949238f,  0.0012626262626262627f,   0.001256281407035176f, //  760 -  796
                 0.00125f,  0.0012437810945273632f,  0.0012376237623762376f,  0.0012315270935960591f,  0.0012254901960784314f,  0.0012195121951219512f,  0.0012135922330097086f,  0.0012077294685990338f,   0.001201923076923077f,  0.0011961722488038277f, //  800 -  836
   0.0011904761904761906f,   0.001184834123222749f,  0.0011792452830188679f,  0.0011737089201877935f,  0.0011682242990654205f,  0.0011627906976744186f,  0.0011574074074074073f,   0.001152073732718894f,  0.0011467889908256881f,   0.001141552511415525f, //  840 -  876
   0.0011363636363636363f,  0.0011312217194570137f,  0.0011261261261261261f,  0.0011210762331838565f,  0.0011160714285714285f,  0.0011111111111111111f,  0.0011061946902654867f,  0.0011013215859030838f,  0.0010964912280701754f,   0.001091703056768559f, //  880 -  916
   0.0010869565217391304f,  0.0010822510822510823f,  0.0010775862068965517f,   0.001072961373390558f,  0.0010683760683760685f,  0.0010638297872340426f,   0.001059322033898305f,  0.0010548523206751054f,  0.0010504201680672268f,  0.0010460251046025104f, //  920 -  956
   0.0010416666666666667f,   0.001037344398340249f,  0.0010330578512396695f,    0.00102880658436214f,  0.0010245901639344263f,  0.0010204081632653062f,  0.0010162601626016261f,  0.0010121457489878543f,  0.0010080645161290322f,   0.001004016064257028f, //  960 -  996
                   0.001f,    0.00099601593625498f,   0.000992063492063492f,  0.0009881422924901185f,   0.000984251968503937f,   0.000980392156862745f,           0.0009765625f,  0.0009727626459143969f,  0.0009689922480620155f,  0.0009652509652509653f, // 1000 - 1036
   0.0009615384615384616f,  0.0009578544061302681f,  0.0009541984732824427f,  0.0009505703422053232f,   0.000946969696969697f,  0.0009433962264150943f,  0.0009398496240601503f,  0.0009363295880149813f,  0.0009328358208955224f,  0.0009293680297397769f, // 1040 - 1076
    0.000925925925925926f,  0.0009225092250922509f,  0.0009191176470588235f,  0.0009157509157509158f,  0.0009124087591240876f,  0.0009090909090909091f,  0.0009057971014492754f,  0.0009025270758122744f,  0.0008992805755395684f,  0.0008960573476702509f, // 1080 - 1116
   0.0008928571428571428f,  0.0008896797153024911f,  0.0008865248226950354f,  0.0008833922261484099f,  0.0008802816901408451f,  0.0008771929824561404f,  0.0008741258741258741f,  0.0008710801393728223f,  0.0008680555555555555f,  0.0008650519031141869f, // 1120 - 1156
   0.0008620689655172414f,   0.000859106529209622f,  0.0008561643835616438f,  0.0008532423208191126f,  0.0008503401360544217f,   0.000847457627118644f,  0.0008445945945945946f,  0.0008417508417508417f,  0.0008389261744966443f,  0.0008361204013377926f, // 1160 - 1196
   0.0008333333333333334f,  0.0008305647840531562f,  0.0008278145695364238f,  0.0008250825082508251f,  0.0008223684210526315f,   0.000819672131147541f,  0.0008169934640522876f,  0.0008143322475570033f,  0.0008116883116883117f,  0.0008090614886731392f, // 1200 - 1236
   0.0008064516129032258f,  0.0008038585209003215f,  0.0008012820512820513f,  0.0007987220447284345f,  0.0007961783439490446f,  0.0007936507936507937f,  0.0007911392405063291f,  0.0007886435331230284f,  0.0007861635220125787f,  0.0007836990595611285f, // 1240 - 1276
              0.00078125f,   0.000778816199376947f,  0.0007763975155279503f,  0.0007739938080495357f,  0.0007716049382716049f,  0.0007692307692307692f,  0.0007668711656441718f,  0.0007645259938837921f,  0.0007621951219512195f,  0.0007598784194528875f, // 1280 - 1316
   0.0007575757575757576f,  0.0007552870090634441f,  0.0007530120481927711f,  0.0007507507507507507f,  0.0007485029940119761f,  0.0007462686567164179f,   0.000744047619047619f,   0.000741839762611276f,  0.0007396449704142012f,  0.0007374631268436578f, // 1320 - 1356
   0.0007352941176470588f,  0.0007331378299120235f,  0.0007309941520467836f,  0.0007288629737609329f,  0.0007267441860465116f,  0.0007246376811594203f,   0.000722543352601156f,  0.0007204610951008645f,  0.0007183908045977011f,  0.0007163323782234957f, // 1360 - 1496
   0.0007142857142857143f,  0.0007122507122507123f,  0.0007102272727272727f,   0.000708215297450425f,  0.0007062146892655367f,  0.0007042253521126761f,  0.0007022471910112359f,  0.0007002801120448179f,  0.0006983240223463687f,  0.0006963788300835655f, // 1400 - 1436
   0.0006944444444444445f,  0.0006925207756232687f,  0.0006906077348066298f,  0.0006887052341597796f,  0.0006868131868131869f,  0.0006849315068493151f,  0.0006830601092896175f,  0.0006811989100817438f,  0.0006793478260869565f,  0.0006775067750677507f, // 1440 - 1476
   0.0006756756756756757f,  0.0006738544474393531f,  0.0006720430107526882f,  0.0006702412868632708f,  0.0006684491978609625f,  0.0006666666666666666f,  0.0006648936170212766f,   0.000663129973474801f,  0.0006613756613756613f,  0.0006596306068601583f, // 1480 - 1516
   0.0006578947368421052f,  0.0006561679790026247f,  0.0006544502617801048f,  0.0006527415143603133f,  0.0006510416666666666f,  0.0006493506493506494f,  0.0006476683937823834f,  0.0006459948320413437f,  0.0006443298969072165f,  0.0006426735218508997f, // 1520 - 1556
    0.000641025641025641f,  0.0006393861892583121f,  0.0006377551020408163f,  0.0006361323155216285f,  0.0006345177664974619f,  0.0006329113924050633f,  0.0006313131313131314f,  0.0006297229219143577f,   0.000628140703517588f,  0.0006265664160401002f, // 1560 - 1596
                0.000625f,  0.0006234413965087282f,  0.0006218905472636816f,  0.0006203473945409429f,  0.0006188118811881188f,  0.0006172839506172839f,  0.0006157635467980296f,  0.0006142506142506142f,  0.0006127450980392157f,  0.0006112469437652812f, // 1600 - 1636
   0.0006097560975609756f,  0.0006082725060827251f,  0.0006067961165048543f,  0.0006053268765133172f,  0.0006038647342995169f,  0.0006024096385542169f,  0.0006009615384615385f,  0.0005995203836930455f,  0.0005980861244019139f,  0.0005966587112171838f, // 1640 - 1676
   0.0005952380952380953f,  0.0005938242280285036f,  0.0005924170616113745f,   0.000591016548463357f,  0.0005896226415094339f,   0.000588235294117647f,  0.0005868544600938967f,   0.000585480093676815f,  0.0005841121495327102f,  0.0005827505827505828f, // 1680 - 1716
   0.0005813953488372093f,   0.000580046403712297f,  0.0005787037037037037f,  0.0005773672055427252f,   0.000576036866359447f,  0.0005747126436781609f,  0.0005733944954128441f,  0.0005720823798627002f,  0.0005707762557077625f,  0.0005694760820045558f, // 1720 - 1756
   0.0005681818181818182f,  0.0005668934240362812f,  0.0005656108597285068f,   0.000564334085778781f,  0.0005630630630630631f,  0.0005617977528089888f,  0.0005605381165919282f,  0.0005592841163310962f,  0.0005580357142857143f,  0.0005567928730512249f, // 1760 - 1796
   0.0005555555555555556f,  0.0005543237250554324f,  0.0005530973451327434f,  0.0005518763796909492f,  0.0005506607929515419f,  0.0005494505494505495f,  0.0005482456140350877f,  0.0005470459518599562f,  0.0005458515283842794f,  0.0005446623093681918f, // 1800 - 1836
   0.0005434782608695652f,  0.0005422993492407809f,  0.0005411255411255411f,  0.0005399568034557236f,  0.0005387931034482759f,  0.0005376344086021505f,   0.000536480686695279f,  0.0005353319057815846f,  0.0005341880341880342f,  0.0005330490405117271f, // 1840 - 1876
   0.0005319148936170213f,  0.0005307855626326964f,  0.0005296610169491525f,  0.0005285412262156448f,  0.0005274261603375527f,  0.0005263157894736842f,  0.0005252100840336134f,  0.0005241090146750524f,  0.0005230125523012552f,  0.0005219206680584551f, // 1880 - 1916
   0.0005208333333333333f,  0.0005197505197505198f,  0.0005186721991701245f,  0.0005175983436853002f,  0.0005165289256198347f,  0.0005154639175257732f,    0.00051440329218107f,   0.000513347022587269f,  0.0005122950819672131f,  0.0005112474437627812f, // 1920 - 1956
   0.0005102040816326531f,  0.0005091649694501018f,  0.0005081300813008131f,  0.0005070993914807302f,  0.0005060728744939271f,   0.000505050505050505f,  0.0005040322580645161f,  0.0005030181086519115f,   0.000502008032128514f,   0.000501002004008016f, // 1960 - 1996
                  0.0005f,   0.000499001996007984f,    0.00049800796812749f,  0.0004970178926441351f,   0.000496031746031746f,  0.0004950495049504951f,  0.0004940711462450593f,  0.0004930966469428008f,  0.0004921259842519685f,  0.0004911591355599214f, // 2000 - 2036
   0.0004901960784313725f,  0.0004892367906066536f,          0.00048828125f,  0.0004873294346978557f, 0.00048638132295719845f,  0.0004854368932038835f, 0.00048449612403100775f, 0.00048355899419729207f, 0.00048262548262548264f,  0.0004816955684007707f, // 2040 - 2076
   0.0004807692307692308f,  0.0004798464491362764f,  0.0004789272030651341f,  0.0004780114722753346f, 0.00047709923664122136f,  0.0004761904761904762f,  0.0004752851711026616f,  0.0004743833017077799f,  0.0004734848484848485f,  0.0004725897920604915f, // 2080 - 2116
   0.0004716981132075472f, 0.00047080979284369113f, 0.00046992481203007516f, 0.00046904315196998124f, 0.00046816479400749064f, 0.00046728971962616824f,  0.0004664179104477612f,  0.0004655493482309125f, 0.00046468401486988845f, 0.00046382189239332097f, // 2120 - 2156
    0.000462962962962963f,  0.0004621072088724584f, 0.00046125461254612545f, 0.00046040515653775324f, 0.00045955882352941176f, 0.00045871559633027525f,  0.0004578754578754579f,  0.0004570383912248629f,  0.0004562043795620438f,  0.0004553734061930783f, // 2160 - 2196
  0.00045454545454545455f, 0.00045372050816696913f,  0.0004528985507246377f,  0.0004520795660036166f,  0.0004512635379061372f, 0.00045045045045045046f,  0.0004496402877697842f,  0.0004488330341113106f, 0.00044802867383512545f,  0.0004472271914132379f, // 2200 - 2236
   0.0004464285714285714f,   0.000445632798573975f, 0.00044483985765124553f,  0.0004440497335701599f,  0.0004432624113475177f,  0.0004424778761061947f, 0.00044169611307420494f,  0.0004409171075837742f, 0.00044014084507042255f,  0.0004393673110720562f, // 2240 - 2276
   0.0004385964912280702f, 0.00043782837127845885f, 0.00043706293706293706f,  0.0004363001745200698f, 0.00043554006968641115f,  0.0004347826086956522f, 0.00043402777777777775f,  0.0004332755632582322f, 0.00043252595155709344f,  0.0004317789291882556f, // 2280 - 2316
   0.0004310344827586207f,  0.0004302925989672978f,   0.000429553264604811f,  0.0004288164665523156f,  0.0004280821917808219f, 0.00042735042735042735f,  0.0004266211604095563f, 0.00042589437819420784f, 0.00042517006802721087f, 0.00042444821731748726f, // 2320 - 2356
    0.000423728813559322f, 0.00042301184433164127f,  0.0004222972972972973f, 0.00042158516020236085f, 0.00042087542087542086f,  0.0004201680672268908f, 0.00041946308724832214f,  0.0004187604690117253f,  0.0004180602006688963f, 0.00041736227045075126f, // 2360 - 2396
   0.0004166666666666667f, 0.00041597337770382697f,  0.0004152823920265781f, 0.00041459369817578774f,  0.0004139072847682119f, 0.00041322314049586776f, 0.00041254125412541255f, 0.00041186161449752884f, 0.00041118421052631577f, 0.00041050903119868636f, // 2400 - 2436
   0.0004098360655737705f,  0.0004091653027823241f,  0.0004084967320261438f,  0.0004078303425774878f, 0.00040716612377850165f,  0.0004065040650406504f, 0.00040584415584415587f,  0.0004051863857374392f,  0.0004045307443365696f,  0.0004038772213247173f, // 2440 - 2476
   0.0004032258064516129f, 0.00040257648953301127f, 0.00040192926045016077f,  0.0004012841091492777f, 0.00040064102564102563f,                 0.0004f, 0.00039936102236421724f, 0.00039872408293460925f,  0.0003980891719745223f,   0.000397456279809221f, // 2480 - 2516
   0.0003968253968253968f,  0.0003961965134706815f, 0.00039556962025316455f, 0.00039494470774091627f,  0.0003943217665615142f,  0.0003937007874015748f, 0.00039308176100628933f,  0.0003924646781789639f, 0.00039184952978056425f,  0.0003912363067292645f, // 2520 - 2556
             0.000390625f,   0.000390015600624025f,  0.0003894080996884735f, 0.00038880248833592535f, 0.00038819875776397513f,  0.0003875968992248062f,  0.0003869969040247678f,  0.0003863987635239567f, 0.00038580246913580245f,  0.0003852080123266564f, // 2560 - 2596
   0.0003846153846153846f, 0.00038402457757296467f,  0.0003834355828220859f, 0.00038284839203675346f, 0.00038226299694189603f,  0.0003816793893129771f, 0.00038109756097560977f,   0.000380517503805175f, 0.00037993920972644377f, 0.00037936267071320183f, // 2600 - 2636
   0.0003787878787878788f, 0.00037821482602118004f, 0.00037764350453172205f,  0.0003770739064856712f, 0.00037650602409638556f, 0.00037593984962406017f, 0.00037537537537537537f,  0.0003748125937031484f, 0.00037425149700598805f,  0.0003736920777279522f, // 2640 - 2676
  0.00037313432835820896f, 0.00037257824143070045f,  0.0003720238095238095f,  0.0003714710252600297f,   0.000370919881305638f, 0.00037037037037037035f,  0.0003698224852071006f, 0.00036927621861152144f,  0.0003687315634218289f,  0.0003681885125184094f, // 2680 - 2716
   0.0003676470588235294f,  0.0003671071953010279f, 0.00036656891495601173f, 0.00036603221083455345f,  0.0003654970760233918f,   0.000364963503649635f, 0.00036443148688046647f,   0.000363901018922853f,  0.0003633720930232558f,   0.000362844702467344f, // 2720 - 2756
  0.00036231884057971015f,   0.000361794500723589f,   0.000361271676300578f, 0.00036075036075036075f, 0.00036023054755043225f, 0.00035971223021582735f, 0.00035919540229885057f,  0.0003586800573888092f, 0.00035816618911174784f,   0.000357653791130186f, // 2760 - 2796
  0.00035714285714285714f,  0.0003566333808844508f, 0.00035612535612535614f, 0.00035561877667140827f,  0.0003551136363636364f,  0.0003546099290780142f,  0.0003541076487252125f,  0.0003536067892503536f, 0.00035310734463276836f,  0.0003526093088857546f, // 2800 - 2836
  0.00035211267605633805f, 0.00035161744022503517f, 0.00035112359550561797f,  0.0003506311360448808f, 0.00035014005602240897f, 0.00034965034965034965f, 0.00034916201117318437f,  0.0003486750348675035f, 0.00034818941504178273f,  0.0003477051460361613f, // 2840 - 2876
  0.00034722222222222224f, 0.00034674063800277393f, 0.00034626038781163435f, 0.00034578146611341634f,  0.0003453038674033149f,  0.0003448275862068965f,  0.0003443526170798898f,   0.000343878954607978f, 0.00034340659340659343f,  0.0003429355281207133f, // 2880 - 2916
  0.00034246575342465754f,  0.0003419972640218878f, 0.00034153005464480874f, 0.00034106412005457026f,  0.0003405994550408719f,  0.0003401360544217687f, 0.00033967391304347825f, 0.00033921302578018993f, 0.00033875338753387534f, 0.00033829499323410016f, // 2920 - 2956
  0.00033783783783783786f, 0.00033738191632928474f, 0.00033692722371967657f, 0.00033647375504710633f,  0.0003360215053763441f,  0.0003355704697986577f,  0.0003351206434316354f, 0.00033467202141900936f, 0.00033422459893048126f,  0.0003337783711615487f, // 2960 - 2996
   0.0003333333333333333f, 0.00033288948069241014f,  0.0003324468085106383f, 0.00033200531208499334f,  0.0003315649867374005f, 0.00033112582781456954f, 0.00033068783068783067f, 0.00033025099075297226f, 0.00032981530343007914f, 0.00032938076416337287f, // 3000 - 3036
   0.0003289473684210526f,   0.000328515111695138f, 0.00032808398950131233f,   0.000327653997378768f,  0.0003272251308900524f,   0.000326797385620915f, 0.00032637075718015666f,  0.0003259452411994785f,  0.0003255208333333333f, 0.00032509752925877764f, // 3040 - 3076
   0.0003246753246753247f, 0.00032425421530479895f,  0.0003238341968911917f,  0.0003234152652005175f, 0.00032299741602067185f,  0.0003225806451612903f, 0.00032216494845360824f, 0.00032175032175032174f, 0.00032133676092544985f, 0.00032092426187419767f, // 3080 - 3116
   0.0003205128205128205f,  0.0003201024327784891f, 0.00031969309462915604f, 0.00031928480204342275f, 0.00031887755102040814f,  0.0003184713375796178f,  0.0003180661577608143f,  0.0003176620076238882f, 0.00031725888324873094f,  0.0003168567807351077f, // 3120 - 3156
  0.00031645569620253165f,  0.0003160556257901391f,  0.0003156565656565657f, 0.00031525851197982345f, 0.00031486146095717883f, 0.00031446540880503143f,   0.000314070351758794f,  0.0003136762860727729f,  0.0003132832080200501f, 0.00031289111389236547f, // 3160 - 3196
               0.0003125f, 0.00031210986267166043f,  0.0003117206982543641f, 0.00031133250311332503f,  0.0003109452736318408f, 0.00031055900621118014f, 0.00031017369727047146f,  0.0003097893432465923f,  0.0003094059405940594f, 0.00030902348578491963f, // 3200 - 3236
  0.00030864197530864197f, 0.00030826140567200987f,  0.0003078817733990148f,  0.0003075030750307503f,  0.0003071253071253071f, 0.00030674846625766873f, 0.00030637254901960784f,  0.0003059975520195838f,  0.0003056234718826406f, 0.00030525030525030525f, // 3240 - 3276
   0.0003048780487804878f,  0.0003045066991473812f, 0.00030413625304136254f, 0.00030376670716889426f, 0.00030339805825242716f, 0.00030303030303030303f,  0.0003026634382566586f,  0.0003022974607013301f, 0.00030193236714975844f, 0.00030156815440289503f, // 3280 - 3316
  0.00030120481927710846f, 0.00030084235860409147f, 0.00030048076923076925f, 0.00030012004801920766f, 0.00029976019184652276f,  0.0002994011976047904f,  0.0002990430622009569f,  0.0002986857825567503f,  0.0002983293556085919f,  0.0002979737783075089f, // 3320 - 3356
  0.00029761904761904765f, 0.00029726516052318666f,  0.0002969121140142518f,  0.0002965599051008304f,  0.0002962085308056872f,  0.0002958579881656805f,  0.0002955082742316785f, 0.00029515938606847696f, 0.00029481132075471697f,  0.0002944640753828033f, // 3360 - 3396
   0.0002941176470588235f,  0.0002937720329024677f, 0.00029342723004694836f, 0.00029308323563892143f,  0.0002927400468384075f, 0.00029239766081871346f,  0.0002920560747663551f, 0.00029171528588098014f,  0.0002913752913752914f,  0.0002910360884749709f, // 3400 - 3436
  0.00029069767441860465f, 0.00029036004645760743f,  0.0002900232018561485f, 0.00028968713789107763f, 0.00028935185185185184f, 0.00028901734104046245f,  0.0002886836027713626f,  0.0002883506343713956f,  0.0002880184331797235f, 0.00028768699654775604f, // 3440 - 3476
  0.00028735632183908046f,  0.0002870264064293915f, 0.00028669724770642203f,   0.000286368843069874f,  0.0002860411899313501f, 0.00028571428571428574f, 0.00028538812785388126f, 0.00028506271379703536f,  0.0002847380410022779f,  0.0002844141069397042f, // 3480 - 3516
   0.0002840909090909091f, 0.00028376844494892167f,  0.0002834467120181406f, 0.00028312570781426955f,  0.0002828054298642534f,  0.0002824858757062147f,  0.0002821670428893905f,  0.0002818489289740699f, 0.00028153153153153153f, 0.00028121484814398203f, // 3520 - 3556
   0.0002808988764044944f,  0.0002805836139169473f,  0.0002802690582959641f,  0.0002799552071668533f,  0.0002796420581655481f, 0.00027932960893854746f, 0.00027901785714285713f,  0.0002787068004459309f, 0.00027839643652561246f, 0.00027808676307007786f, // 3560 - 3596
   0.0002777777777777778f,  0.0002774694783573807f,  0.0002771618625277162f,  0.0002768549280177187f,  0.0002765486725663717f, 0.00027624309392265195f,  0.0002759381898454746f,  0.0002756339581036384f, 0.00027533039647577095f,   0.000275027502750275f, // 3600 - 3636
   0.0002747252747252747f, 0.00027442371020856203f, 0.00027412280701754384f,  0.0002738225629791895f,  0.0002735229759299781f,   0.000273224043715847f,  0.0002729257641921397f,  0.0002726281352235551f,  0.0002723311546840959f,  0.0002720348204570185f, // 3640 - 3676
   0.0002717391304347826f,  0.0002714440825190011f, 0.00027114967462039046f, 0.00027085590465872155f, 0.00027056277056277056f,  0.0002702702702702703f,  0.0002699784017278618f, 0.00026968716289104636f, 0.00026939655172413793f, 0.00026910656620021526f, // 3680 - 3716
  0.00026881720430107527f,  0.0002685284640171858f,  0.0002682403433476395f,  0.0002679528403001072f,  0.0002676659528907923f, 0.00026737967914438503f,  0.0002670940170940171f, 0.00026680896478121667f, 0.00026652452025586353f, 0.00026624068157614486f, // 3720 - 3756
  0.00026595744680851064f, 0.00026567481402763017f,  0.0002653927813163482f,  0.0002651113467656416f, 0.00026483050847457627f, 0.00026455026455026457f,  0.0002642706131078224f, 0.00026399155227032733f, 0.00026371308016877635f, 0.00026343519494204424f, // 3760 - 3796
   0.0002631578947368421f,  0.0002628811777076761f,  0.0002626050420168067f, 0.00026232948583420777f,  0.0002620545073375262f,  0.0002617801047120419f,  0.0002615062761506276f, 0.00026123301985370953f, 0.00026096033402922753f, 0.00026068821689259646f, // 3800 - 3836
  0.00026041666666666666f, 0.00026014568158168577f,  0.0002598752598752599f, 0.00025960539979231567f, 0.00025933609958506224f, 0.00025906735751295336f,  0.0002587991718426501f,  0.0002585315408479835f, 0.00025826446280991736f, 0.00025799793601651185f, // 3840 - 3876
   0.0002577319587628866f, 0.00025746652935118434f,   0.000257201646090535f, 0.00025693730729701953f,  0.0002566735112936345f,  0.0002564102564102564f, 0.00025614754098360657f, 0.00025588536335721597f,  0.0002556237218813906f,  0.0002553626149131767f, // 3880 - 3916
  0.00025510204081632655f, 0.00025484199796126404f,  0.0002545824847250509f,   0.000254323499491353f, 0.00025406504065040653f,  0.0002538071065989848f,  0.0002535496957403651f, 0.00025329280648429586f, 0.00025303643724696357f,  0.0002527805864509606f, // 3920 - 3956
   0.0002525252525252525f,  0.0002522704339051463f, 0.00025201612903225806f, 0.00025176233635448137f, 0.00025150905432595576f,  0.0002512562814070352f,   0.000251004016064257f, 0.00025075225677031093f,   0.000250501002004008f, 0.00025025025025025025f, // 3960 - 3996
                 0.00025f, 0.00024975024975024975f,   0.000249500998003992f,  0.0002492522432701894f,   0.000249003984063745f,  0.0002487562189054726f, 0.00024850894632206757f, 0.00024826216484607745f,   0.000248015873015873f,  0.0002477700693756194f, // 4000 - 4036
  0.00024752475247524753f,  0.0002472799208704253f, 0.00024703557312252963f, 0.00024679170779861795f,  0.0002465483234714004f,  0.0002463054187192118f, 0.00024606299212598425f,  0.0002458210422812193f,  0.0002455795677799607f,  0.0002453385672227674f, // 4040 - 4076
  0.00024509803921568627f,  0.0002448579823702253f,  0.0002446183953033268f, 0.00024437927663734115f,         0.000244140625f                                                                                                                               // 4080 - 4096
};

/**
@brief: ker_layer_norm
Standard layer normalization.
It will not only output the layer norm result,
  but also outputs variance.
  may also output means, depends on whether
  the means argument is nullptr

@thread
gridDim.x = batch_size * seq_len
blockDim.x = min(hidden_size, MAX_THREADS)

@param
ln_res: [batch_size * seq_len, hidden_size], ln result.
vars: [batch_size * seq_len], variance per token
means: [batch_size * seq_len], means per token, can be nullput
inp: [batch_size * seq_len, hidden_size], ln input.
scale: [hidden_size], ln scale
bias: [hidden_size], ln bias
*/
template <typename T>
__global__ void ker_layer_norm(T *ln_res, T *vars, T *means, const T *inp,
                               const T *scale, const T *bias, int hidden_size) {

  /// BEGIN ASSIGN4_2_1
  /// TODO
  // Hints:
  // 1. Compute x and x^2 with reinterpret_cast by casting to float4 for speedup
  // 2. Compute reduce sum with blockReduce and add epsilon with LN_EPSILON
  // 3. Compute layernorm result with reinterpret_cast by casting to float4 for speedup

  // Step 1
  float l_sums[2] = {0};
  __shared__ float sums[2];
  const float4 *inp_f4 = reinterpret_cast<const float4 *>(inp) + blockIdx.x * hidden_size;
  for (uint idx = threadIdx.x; idx < hidden_size; idx += blockDim.x) {
    const float4  val = inp_f4[idx];
    l_sums[0] += val.x + val.y + val.z + val.w;
    l_sums[1] += val.x * val.x + val.y * val.y + val.z * val.z + val.w * val.w;
  }

  // Step 2
  blockReduce<ReduceType::kSum, 2>(l_sums);
  if (threadIdx.x == 0) {
    sums[0] = l_sums[0];
    sums[1] = l_sums[1];
  }
  __syncthreads();
  const float  mean_x = sums[0] / (hidden_size << 2);
  const float mean_x2 = sums[1] / (hidden_size << 2);
  const float variance = mean_x2 - mean_x * mean_x + LN_EPSILON;
  const float sigma = sqrtf(variance);

  // Step 3
  if (threadIdx.x == 0) {
    if (means) means[blockIdx.x] = mean_x;
    vars[blockIdx.x] = variance;
  }
  float4 *ln_res_f4 = reinterpret_cast<float4 *>(ln_res) + blockIdx.x * hidden_size;
  const float4 *scale_f4 = reinterpret_cast<const float4 *>(scale);
  const float4  *bias_f4 = reinterpret_cast<const float4 *>( bias);
  for (uint idx = threadIdx.x; idx < hidden_size; idx += blockDim.x) {
    const float4 val = inp_f4[idx];
    const float4 scale_i = scale_f4[idx];
    const float4 bias_i = bias_f4[idx];
    ln_res_f4[idx] = make_float4(
      scale_i.x * (val.x - mean_x) / sigma + bias_i.x,
      scale_i.y * (val.y - mean_x) / sigma + bias_i.y,
      scale_i.z * (val.z - mean_x) / sigma + bias_i.z,
      scale_i.w * (val.w - mean_x) / sigma + bias_i.w
    );
  }
  /// END ASSIGN4_2_1
}

extern "C" {
void launch_layernorm(float *ln_res, float *vars, float *means,
                              const float *inp, const float *scale,
                              const float *bias, int batch_size, int hidden_dim,
                              cudaStream_t stream) {
  if (hidden_dim % 4 != 0) {
    throw std::runtime_error("violate hidden_dim % 4 = 0");
  }
  int float_size = sizeof(float);
  int input_size = batch_size * hidden_dim * float_size;
  int scale_size = hidden_dim * float_size;
  int bias_size = hidden_dim * float_size;
  int output_size = batch_size * hidden_dim * float_size;
  int mean_size = batch_size * float_size;
  int var_size = batch_size * float_size;


  float *d_ln_res, *d_vars, *d_means, *d_inp, *d_scale, *d_bias;
  cudaMalloc((void **)&d_ln_res, output_size);
  cudaMalloc((void **)&d_vars, var_size);
  cudaMalloc((void **)&d_means, mean_size);
  cudaMalloc((void **)&d_inp, input_size);
  cudaMalloc((void **)&d_scale, scale_size);
  cudaMalloc((void **)&d_bias, bias_size);

  cudaMemcpy(d_inp, inp, input_size, cudaMemcpyHostToDevice);
  cudaMemcpy(d_scale, scale, scale_size, cudaMemcpyHostToDevice);
  cudaMemcpy(d_bias, bias, bias_size, cudaMemcpyHostToDevice);

  // For using float4
  hidden_dim >>= 2;
  int nthread = min(((hidden_dim + 31) / 32) * 32, MAX_THREADS);
  dim3 grid_dim(batch_size);
  dim3 block_dim(nthread);
  ker_layer_norm<float><<<grid_dim, block_dim, 0, stream>>>(
    d_ln_res, d_vars, d_means, d_inp, d_scale, d_bias, hidden_dim);

  // Copy back to the host
  cudaMemcpy(ln_res, d_ln_res, output_size, cudaMemcpyDeviceToHost);
  cudaMemcpy(vars, d_vars, var_size, cudaMemcpyDeviceToHost);
  cudaMemcpy(means, d_means, mean_size, cudaMemcpyDeviceToHost);
  cudaDeviceSynchronize();

  // Check CUDA execution
  cudaError_t err = cudaGetLastError();
  if (err != cudaSuccess) {
    fprintf(stderr, "launch_layernorm Error: %s\n", cudaGetErrorString(err));
    // Handle the error (e.g., by exiting the program)
    exit(EXIT_FAILURE);
  }

  // Free memory on device
  cudaFree(d_ln_res);
  cudaFree(d_vars);
  cudaFree(d_means);
  cudaFree(d_inp);
  cudaFree(d_scale);
  cudaFree(d_bias);

}
}

/**
@brief: ker_ln_bw_dgamma_dbetta
Layer norm backword kernel, compute the gradient of gamma and betta.
dbetta = sum(dout, dim=0)
dgamma = sum(xhat * dout, dim=0)
xhat = (input - mean) * rsqrt(var) or
  (output - betta) / gamma

@thread
gridDim.x = hidden_size / 32
blockDim.x = 32
blockDim.y = 32

@param
gamma_grad: [hidden_size], gradient of gamma
betta_grad: [hidden_size], gradient of betta
out_grad: [batch_size * seq_len, hidden_size], gradient of betta ln output
inp_or_out: [batch_size * seq_len, hidden_size], ln output if means is nullptr
  ln input if means is not nullptr
gamma: [hidden_size], gamma of ln,
  used to compute xhat, maybe nullptr
betta: [hidden_size], betta of ln,
  used to compute xhat, maybe nullptr
vars: [batch_size * seq_len], variance of ln forward,
  used to compute xhat, maybe nullptr
means: [batch_size * seq_len], mean of ln forward,
  used to compute xhat, maybe nullptr
(gamma && betta) ^ (vars && means) should be true
*/
template <typename T>
__global__ void ker_ln_bw_dgamma_dbetta(T *gamma_grad, T *betta_grad,
                                        const T *out_grad,
                                        const T *inp, const T *gamma,
                                        const T *betta, const T *vars,
                                        const T *means, int rows, int width) {

  /// BEGIN ASSIGN4_2_2
  /// TODO
  // Hints:
  // 1. Compute the partial gradients by looping across inp rows
  // 2. Store the partial gradients in the shared memory arrays
  // 3. Compute the reduce sum of the shared memory arrays with g.shfl_down
  //      -> More hints about `g.shfl_down`:
  //      -> https://developer.nvidia.com/blog/cooperative-groups/#:~:text=Using%20thread_block_tile%3A%3Ashfl_down()%20to%20simplify%20our%20warp%2Dlevel%20reduction%20does%20benefit%20our%20code%3A%20it%20simplifies%20it%20and%20eliminates%20the%20need%20for%20shared%20memory
  //      -> The highlighted line gives you a conceptual understanding of what the g.shfl_down is doing. Usually, the threads inside a block need to load everything to shared memory and work together to reduce the result (like what you have implemented in the hw1 for reduce function).
  //      -> Now g.shfl_down helps you do so without consuming any shared memory. g.shfl_down makes it more efficient.
  // 4. Assign the final result to the correct position in the global output

  __shared__ float betta_buffer[TILE_DIM][TILE_DIM];
  __shared__ float gamma_buffer[TILE_DIM][TILE_DIM];

  if (blockIdx.y) return;

  const uint idx_x = blockDim.x * blockIdx.x + threadIdx.x;
  const uint idx_y = threadIdx.y;
  const uint size = rows * width;
  inp += idx_y * width;
  out_grad += idx_y * width;

  cg::thread_block b = cg::this_thread_block();
  cg::thread_block_tile<TILE_DIM> g = cg::tiled_partition<TILE_DIM>(b);

  // Step 1
  const uint stride = blockDim.y * width;
  uint i = idx_x;
  T l_d_gam = 0;
  T l_d_bet = 0;
  for (uint i_y = idx_y; i_y < rows; i_y += blockDim.y) {
    if (i >= size) break;
    l_d_gam += out_grad[i] * (inp[i] - means[i_y]) * rsqrt(vars[i_y] + LN_EPSILON);
    l_d_bet += out_grad[i];
    i += stride;
  }

  // Step 2
  betta_buffer[threadIdx.x][threadIdx.y] = l_d_bet;
  gamma_buffer[threadIdx.x][threadIdx.y] = l_d_gam;
  __syncthreads();

  // Step 3
  l_d_bet = betta_buffer[threadIdx.y][threadIdx.x];
  l_d_gam = gamma_buffer[threadIdx.y][threadIdx.x];
  for (int i = g.size() / 2; i > 0; i /= 2) {
    l_d_gam += g.shfl_down(l_d_gam, i);
    l_d_bet += g.shfl_down(l_d_bet, i);
  }
  if (!threadIdx.x) {
    betta_buffer[threadIdx.y][threadIdx.x] = l_d_bet;
    gamma_buffer[threadIdx.y][threadIdx.x] = l_d_gam;
  }
  __syncthreads();

  // Step 4
  if (idx_y) return;
  if (idx_x >= width) return;
  gamma_grad[idx_x] = gamma_buffer[threadIdx.x][threadIdx.y];
  betta_grad[idx_x] = betta_buffer[threadIdx.x][threadIdx.y];
  /// END ASSIGN4_2_2
}

/**
@brief: ker_ln_bw_dinp
Layer norm backword kernel, compute the gradient of input.
dinp = (dxhat - (sum(dxhat) + xhat * sum(dxhat * xhat)) / hidden_dim)
  * rsqrt(var)
xhat = (input - mean) * rsqrt(var) if mean is not nullptr
       (output - betta) / gamma if mean is nullptr
dxhat = dout * gamma


@thread
gridDim.x = batch_size * seq_len
blockDim.x = hidden_size

@param
inp_grad: [batch_size * seq_len, hidden_size], gradient of betta ln output
out_grad: [batch_size * seq_len, hidden_size], gradient of betta ln output
residual_grad: [batch_size * seq_len, hidden_size], gradient of residual input,
  usually appear in pre-layer-norm for transformer layer, maybe nullptr
inp_or_out: [batch_size * seq_len, hidden_size], ln output if means is nullptr
  ln input if means is not nullptr
gamma: [hidden_size], gamma of ln,
  used to compute xhat and dxhat
betta: [hidden_size], betta of ln,
  used to compute xhat, maybe nullptr
vars: [batch_size * seq_len], variance of ln forward,
  used to compute xhat and dinp
means: [batch_size * seq_len], mean of ln forward,
  used to compute xhat, maybe nullptr
*/
template <typename T>
__global__ void ker_ln_bw_dinp(T *inp_grad, const T *out_grad, const T *inp,
                               const T *gamma, const T *betta, const T *vars,
                               const T *means, int hidden_dim) {

  /// BEGIN ASSIGN4_2_2
  /// TODO
  // Hints:
  // 1. Compute dxhat=dy*w with reinterpret_cast by casting to float4 for speedup
  // 2. Compute xhat with reinterpret_cast by casting to float4 for speedup
  // 3. Compute reduce sum for dxhat and dxhat*xhat with blockReduce
  // 4. Compute final gradient

  if (threadIdx.x >= hidden_dim) return;

  const uint idx_y = blockIdx.x;
  const uint offset = idx_y * hidden_dim;
  const float4 *inp_f4 = reinterpret_cast<const float4 *>(inp) + offset;
  const T mean = means[idx_y];
  const T rstd = rsqrt(vars[idx_y] + LN_EPSILON);
  const float4 *out_grad_f4 = reinterpret_cast<const float4 *>(out_grad) + offset;
  const float4 *gamma_f4 = reinterpret_cast<const float4 *>(gamma);
  float4 *inp_grad_f4 = reinterpret_cast<float4 *>(inp_grad) + offset;

  const uint idx = threadIdx.x;
  float l_sums[2];
  __shared__ float sums[2];

  const float4 y_j = out_grad_f4[idx];
  const float4 gamma_j = gamma_f4[idx];
  const float4 dxhat = make_float4(
    y_j.x * gamma_j.x,
    y_j.y * gamma_j.y,
    y_j.z * gamma_j.z,
    y_j.w * gamma_j.w
  );

  // Step 2
  const float4 inp_j = inp_f4[idx];
  const float4 xhat = make_float4(
    (inp_j.x - mean) * rstd,
    (inp_j.y - mean) * rstd,
    (inp_j.z - mean) * rstd,
    (inp_j.w - mean) * rstd
  );

  // Step 3
  l_sums[0] = dxhat.x + dxhat.y + dxhat.z + dxhat.w;
  l_sums[1] = xhat.x * dxhat.x + xhat.y * dxhat.y + xhat.z * dxhat.z + xhat.w * dxhat.w;

  blockReduce<ReduceType::kSum, 2>(l_sums);
  if (!threadIdx.x) {
    sums[0] = l_sums[0];
    sums[1] = l_sums[1];
    //printf("sums row %d: dxhat %f xhat_dxhat %f\n", blockIdx.x, sums[0], sums[1]);
  }
  __syncthreads();
  //printf("thread %d %d xhat [%f, %f, %f, %f] dxhat [%f, %f, %f, %f]\n", blockIdx.x, threadIdx.x, xhat.x, xhat.y, xhat.z, xhat.w, dxhat.x, dxhat.y, dxhat.z, dxhat.w);

  // Step 4
  // because m is small, use a lookup table to avoid a divide
  const float one_over_m = reciprocals[hidden_dim];
  float sum_dxhat_m = sums[0] * one_over_m;
  float sum_xhat_dxhat_m = sums[1] * one_over_m;
  inp_grad_f4[idx] = make_float4(
    (dxhat.x - sum_dxhat_m - xhat.x * sum_xhat_dxhat_m) * rstd,
    (dxhat.y - sum_dxhat_m - xhat.y * sum_xhat_dxhat_m) * rstd,
    (dxhat.z - sum_dxhat_m - xhat.z * sum_xhat_dxhat_m) * rstd,
    (dxhat.w - sum_dxhat_m - xhat.w * sum_xhat_dxhat_m) * rstd
  );
  /// END ASSIGN4_2_2
}
// WARNING: because of the way the loops below have been unrolled, this needs to be launched with the exact iteration count.
template <typename T, int ITERATIONS>
__global__ void ker_ln_bw_dinp_gt4096(T *inp_grad, const T *out_grad, const T *inp,
                               const T *gamma, const T *betta, const T *vars,
                               const T *means, int hidden_dim) {
  // Here we will allocate dynamic memory for xhat and dxhat on the stack
  // The stack is fairly generous but it is limited, so we probably wouldn't want to use this kernel with iterations > 1000 or so
  float4 xhat[ITERATIONS];
  float4 dxhat[ITERATIONS];

  const uint idx_y = blockIdx.x;
  const uint offset = idx_y * hidden_dim;
  const float4 *inp_f4 = reinterpret_cast<const float4 *>(inp) + offset;
  const T mean = means[idx_y];
  const T rstd = rsqrt(vars[idx_y] + LN_EPSILON);
  const float4 *out_grad_f4 = reinterpret_cast<const float4 *>(out_grad) + offset;
  const float4 *gamma_f4 = reinterpret_cast<const float4 *>(gamma);
  float4 *inp_grad_f4 = reinterpret_cast<float4 *>(inp_grad) + offset;

  const uint idx = threadIdx.x;
  float l_sums[2];
  __shared__ float sums[2];
  uint i = idx;
  uint k = 0;
  #pragma unroll
  for (; k < ITERATIONS - 1; k++) {
    // Step 1
    const float4 y_j = out_grad_f4[i];
    const float4 gamma_j = gamma_f4[i];
    dxhat[k] = make_float4(
      y_j.x * gamma_j.x,
      y_j.y * gamma_j.y,
      y_j.z * gamma_j.z,
      y_j.w * gamma_j.w
    );

    // Step 2
    const float4 inp_j = inp_f4[i];
    xhat[k] = make_float4(
      (inp_j.x - mean) * rstd,
      (inp_j.y - mean) * rstd,
      (inp_j.z - mean) * rstd,
      (inp_j.w - mean) * rstd
    );

    // Step 3
    l_sums[0] = dxhat[k].x + dxhat[k].y + dxhat[k].z + dxhat[k].w;
    l_sums[1] = xhat[k].x * dxhat[k].x + xhat[k].y * dxhat[k].y + xhat[k].z * dxhat[k].z + xhat[k].w * dxhat[k].w;
    i += blockDim.x;
  }
  // manually unroll the last iteration to avoid a branch in the loop
  if (i < hidden_dim) {
    // Step 1
    const float4 y_j = out_grad_f4[i];
    const float4 gamma_j = gamma_f4[i];
    dxhat[k] = make_float4(
      y_j.x * gamma_j.x,
      y_j.y * gamma_j.y,
      y_j.z * gamma_j.z,
      y_j.w * gamma_j.w
    );

    // Step 2
    const float4 inp_j = inp_f4[i];
    xhat[k] = make_float4(
      (inp_j.x - mean) * rstd,
      (inp_j.y - mean) * rstd,
      (inp_j.z - mean) * rstd,
      (inp_j.w - mean) * rstd
    );

    // Step 3
    l_sums[0] = dxhat[k].x + dxhat[k].y + dxhat[k].z + dxhat[k].w;
    l_sums[1] = xhat[k].x * dxhat[k].x + xhat[k].y * dxhat[k].y + xhat[k].z * dxhat[k].z + xhat[k].w * dxhat[k].w;
  }

  blockReduce<ReduceType::kSum, 2>(l_sums);
  if (!threadIdx.x) {
    sums[0] = l_sums[0];
    sums[1] = l_sums[1];
  }
  __syncthreads();

  // Step 4
  // use a lookup table instead of a divide here
  const float one_over_m = reciprocals[hidden_dim];
  float sum_dxhat_m = sums[0] * one_over_m;
  float sum_xhat_dxhat_m = sums[1] * one_over_m;
  i = idx;
  k = 0;
  #pragma unroll
  for (; k < ITERATIONS - 1; k++) {
    inp_grad_f4[i] = make_float4(
      (dxhat[k].x - sum_dxhat_m - xhat[k].x * sum_xhat_dxhat_m) * rstd,
      (dxhat[k].y - sum_dxhat_m - xhat[k].y * sum_xhat_dxhat_m) * rstd,
      (dxhat[k].z - sum_dxhat_m - xhat[k].z * sum_xhat_dxhat_m) * rstd,
      (dxhat[k].w - sum_dxhat_m - xhat[k].w * sum_xhat_dxhat_m) * rstd
    );
    i += blockDim.x;
  }
  // manually unroll the last loop because of early returns
  if (i >= hidden_dim) return;
  inp_grad_f4[i] = make_float4(
    (dxhat[k].x - sum_dxhat_m - xhat[k].x * sum_xhat_dxhat_m) * rstd,
    (dxhat[k].y - sum_dxhat_m - xhat[k].y * sum_xhat_dxhat_m) * rstd,
    (dxhat[k].z - sum_dxhat_m - xhat[k].z * sum_xhat_dxhat_m) * rstd,
    (dxhat[k].w - sum_dxhat_m - xhat[k].w * sum_xhat_dxhat_m) * rstd
  );
  /// END ASSIGN4_2_2
}
template <typename T>
__global__ void ker_ln_bw_dinp_gt16384(T *inp_grad, const T *out_grad, const T *inp,
                               const T *gamma, const T *betta, const T *vars,
                               const T *means, int hidden_dim) {
  // Here we will allocate dynamic memory for xhat and dxhat on the heap
  // This should be large enough that we will have other problems if we run out
  const uint ITERATIONS = hidden_dim / MAX_THREADS;
  float4 *xhat = (float4*) malloc(ITERATIONS * sizeof(float4));
  float4 *dxhat = (float4*) malloc(ITERATIONS * sizeof(float4));

  const uint idx_y = blockIdx.x;
  const uint offset = idx_y * hidden_dim;
  const float4 *inp_f4 = reinterpret_cast<const float4 *>(inp) + offset;
  const T mean = means[idx_y];
  const T rstd = rsqrt(vars[idx_y] + LN_EPSILON);
  const float4 *out_grad_f4 = reinterpret_cast<const float4 *>(out_grad) + offset;
  const float4 *gamma_f4 = reinterpret_cast<const float4 *>(gamma);
  float4 *inp_grad_f4 = reinterpret_cast<float4 *>(inp_grad) + offset;

  const uint idx = threadIdx.x;
  float l_sums[2];
  __shared__ float sums[2];
  uint k = 0;
  for (uint i = idx; i < hidden_dim; i += blockDim.x) {
    // Step 1
    const float4 y_j = out_grad_f4[i];
    const float4 gamma_j = gamma_f4[i];
    dxhat[k] = make_float4(
      y_j.x * gamma_j.x,
      y_j.y * gamma_j.y,
      y_j.z * gamma_j.z,
      y_j.w * gamma_j.w
    );

    // Step 2
    const float4 inp_j = inp_f4[i];
    xhat[k] = make_float4(
      (inp_j.x - mean) * rstd,
      (inp_j.y - mean) * rstd,
      (inp_j.z - mean) * rstd,
      (inp_j.w - mean) * rstd
    );

    // Step 3
    l_sums[0] = dxhat[k].x + dxhat[k].y + dxhat[k].z + dxhat[k].w;
    l_sums[1] = xhat[k].x * dxhat[k].x + xhat[k].y * dxhat[k].y + xhat[k].z * dxhat[k].z + xhat[k].w * dxhat[k].w;
    k += 1;
  }

  blockReduce<ReduceType::kSum, 2>(l_sums);
  if (!threadIdx.x) {
    sums[0] = l_sums[0];
    sums[1] = l_sums[1];
  }
  __syncthreads();

  // Step 4
  // perform only one divide here to avoid repeated divides below
  float rm = 1.f / (float) (hidden_dim << 2);
  float sum_dxhat_m = sums[0] * rm;
  float sum_xhat_dxhat_m = sums[1] * rm;
  k = 0;
  for (uint i = idx; i < hidden_dim; i += blockDim.x) {
    inp_grad_f4[i] = make_float4(
      (dxhat[k].x - sum_dxhat_m - xhat[k].x * sum_xhat_dxhat_m) * rstd,
      (dxhat[k].y - sum_dxhat_m - xhat[k].y * sum_xhat_dxhat_m) * rstd,
      (dxhat[k].z - sum_dxhat_m - xhat[k].z * sum_xhat_dxhat_m) * rstd,
      (dxhat[k].w - sum_dxhat_m - xhat[k].w * sum_xhat_dxhat_m) * rstd
    );
    k += 1;
  }
  free(xhat);
  free(dxhat);
  /// END ASSIGN4_2_2
}
extern "C" {
void launch_layernorm_bw(float *gamma_grad, float *betta_grad, float *inp_grad,
                         const float *out_grad, const float *inp, const float *gamma,
                         const float *betta, const float *vars,
                         const float *means, int batch_size, int hidden_dim,
                         cudaStream_t stream_1, cudaStream_t stream_2) {

  // Allocate device memory
  float *d_gamma_grad, *d_betta_grad, *d_inp_grad, *d_out_grad, *d_inp, *d_gamma, *d_betta, *d_vars, *d_means;
  int grad_output_size = batch_size * hidden_dim * sizeof(float);
  int gamma_betta_size = hidden_dim * sizeof(float);
  int vars_means_size = batch_size * sizeof(float);

  cudaMalloc((void **)&d_gamma_grad, gamma_betta_size);
  cudaMalloc((void **)&d_betta_grad, gamma_betta_size);
  cudaMalloc((void **)&d_inp_grad, grad_output_size);
  cudaMalloc((void **)&d_out_grad, grad_output_size);
  cudaMalloc((void **)&d_inp, grad_output_size);
  cudaMalloc((void **)&d_gamma, gamma_betta_size);
  cudaMalloc((void **)&d_betta, gamma_betta_size);
  cudaMalloc((void **)&d_vars, vars_means_size);
  cudaMalloc((void **)&d_means, vars_means_size);

  // Copy memory to device
  cudaMemcpy((void *)d_out_grad, out_grad, grad_output_size, cudaMemcpyHostToDevice);
  cudaMemcpy((void *)d_inp, inp, grad_output_size, cudaMemcpyHostToDevice);
  cudaMemcpy((void *)d_gamma, gamma, gamma_betta_size, cudaMemcpyHostToDevice);
  cudaMemcpy((void *)d_betta, betta, gamma_betta_size, cudaMemcpyHostToDevice);
  cudaMemcpy((void *)d_vars, vars, vars_means_size, cudaMemcpyHostToDevice);
  cudaMemcpy((void *)d_means, means, vars_means_size, cudaMemcpyHostToDevice);

  // Launch kernels
  // Compute grad of gamma and betta
  // This calculates the number of blocks needed to cover the data along the specified dimension, rounds it up.
  dim3 grid_dim((hidden_dim + TILE_DIM - 1) / TILE_DIM);
  dim3 block_dim(TILE_DIM, TILE_DIM);
  ker_ln_bw_dgamma_dbetta<float><<<grid_dim, block_dim, 0, stream_1>>>(
      d_gamma_grad, d_betta_grad, d_out_grad, d_inp, d_gamma, d_betta, d_vars,
      d_means, batch_size, hidden_dim);

  // Compute grad of input
  if (hidden_dim % 4 != 0) {
    throw std::runtime_error("hidden_dim % 4 != 0");
  }
  hidden_dim >>= 2;
  int nthread = min(((hidden_dim + 31) / 32) * 32, MAX_THREADS);
  if (hidden_dim <= 4096) {
    ker_ln_bw_dinp<float><<<batch_size, nthread, 0, stream_2>>>(
      d_inp_grad, d_out_grad, d_inp, d_gamma, d_betta, d_vars, d_means, hidden_dim);
  } else if (hidden_dim <= 8192) {
    ker_ln_bw_dinp_gt4096<float, 2><<<batch_size, nthread, 0, stream_2>>>(
      d_inp_grad, d_out_grad, d_inp, d_gamma, d_betta, d_vars, d_means, hidden_dim);
  } else if (hidden_dim <= 12288) {
    ker_ln_bw_dinp_gt4096<float, 3><<<batch_size, nthread, 0, stream_2>>>(
      d_inp_grad, d_out_grad, d_inp, d_gamma, d_betta, d_vars, d_means, hidden_dim);
  } else if (hidden_dim <= 16384) {
    ker_ln_bw_dinp_gt4096<float, 4><<<batch_size, nthread, 0, stream_2>>>(
      d_inp_grad, d_out_grad, d_inp, d_gamma, d_betta, d_vars, d_means, hidden_dim);
  } else {
    ker_ln_bw_dinp_gt16384<float><<<batch_size, nthread, 0, stream_2>>>(
      d_inp_grad, d_out_grad, d_inp, d_gamma, d_betta, d_vars, d_means, hidden_dim);
  }

  // Synchronize and check for errors
  cudaDeviceSynchronize();
  cudaError_t err = cudaGetLastError();
  if (err != cudaSuccess) {
    fprintf(stderr, "launch_layernorm_bw Error: %s\n", cudaGetErrorString(err));
    exit(EXIT_FAILURE);
  }

  // Copy back to host
  cudaMemcpy(gamma_grad, d_gamma_grad, gamma_betta_size, cudaMemcpyDeviceToHost);
  cudaMemcpy(betta_grad, d_betta_grad, gamma_betta_size, cudaMemcpyDeviceToHost);
  cudaMemcpy(inp_grad, d_inp_grad, grad_output_size, cudaMemcpyDeviceToHost);

  // Free device memory
  cudaFree(d_gamma_grad);
  cudaFree(d_betta_grad);
  cudaFree(d_inp_grad);
  cudaFree((void *)d_out_grad);
  cudaFree((void *)d_inp);
  cudaFree((void *)d_gamma);
  cudaFree((void *)d_betta);
  cudaFree((void *)d_vars);
  cudaFree((void *)d_means);
}}
}}
